- returnnnnnnnnnnnnnnnnn:106)"(double*, double*, double*, i32, i32, i32, double) {
merge:
  %7 = insertelement <1 x double> undef, double %6, i32 0
  %8 = bitcast double* %1 to <1 x double>*
  %9 = getelementptr double* %1, i64 1
  %10 = bitcast double* %9 to <1 x double>*
  %11 = getelementptr double* %1, i64 2
  %12 = bitcast double* %11 to <1 x double>*
  %13 = sext i32 %4 to i64
  %14 = getelementptr double* %1, i64 %13
  %15 = bitcast double* %14 to <1 x double>*
  %16 = add i32 %4, 1
  %17 = sext i32 %16 to i64
  %18 = getelementptr double* %1, i64 %17
  %19 = bitcast double* %18 to <1 x double>*
  %20 = add i32 %4, 2
  %21 = sext i32 %20 to i64
  %22 = getelementptr double* %1, i64 %21
  %23 = bitcast double* %22 to <1 x double>*
  %24 = shl i32 %4, 1
  %25 = sext i32 %24 to i64
  %26 = getelementptr double* %1, i64 %25
  %27 = bitcast double* %26 to <1 x double>*
  %28 = or i32 %24, 1
  %29 = sext i32 %28 to i64
  %30 = getelementptr double* %1, i64 %29
  %31 = bitcast double* %30 to <1 x double>*
  %32 = add i32 %24, 2
  %33 = sext i32 %32 to i64
  %34 = getelementptr double* %1, i64 %33
  %35 = bitcast double* %34 to <1 x double>*
  %36 = shl i32 %5, 1
  %37 = mul i32 %5, 3
  %38 = shl i32 %5, 2
  %39 = sext i32 %36 to i64
  %40 = sext i32 %38 to i64
  %41 = sext i32 %5 to i64
  %42 = sext i32 %37 to i64
  %43 = bitcast double* %0 to <1 x double>*
  %44 = load <1 x double>* %43, align 8
  %45 = getelementptr double* %0, i64 1
  %46 = bitcast double* %45 to <1 x double>*
  %47 = load <1 x double>* %46, align 8
  %48 = getelementptr double* %0, i64 2
  %49 = bitcast double* %48 to <1 x double>*
  %50 = load <1 x double>* %49, align 8
  %51 = getelementptr double* %0, i64 3
  %52 = bitcast double* %51 to <1 x double>*
  %53 = load <1 x double>* %52, align 8
  %54 = getelementptr double* %0, i64 4
  %55 = bitcast double* %54 to <1 x double>*
  %56 = load <1 x double>* %55, align 8
  %57 = getelementptr double* %0, i64 %41
  %58 = bitcast double* %57 to <1 x double>*
  %59 = load <1 x double>* %58, align 8
  %60 = add i32 %5, 1
  %61 = sext i32 %60 to i64
  %62 = getelementptr double* %0, i64 %61
  %63 = bitcast double* %62 to <1 x double>*
  %64 = load <1 x double>* %63, align 8
  %65 = add i32 %5, 2
  %66 = sext i32 %65 to i64
  %67 = getelementptr double* %0, i64 %66
  %68 = bitcast double* %67 to <1 x double>*
  %69 = load <1 x double>* %68, align 8
  %70 = add i32 %5, 3
  %71 = sext i32 %70 to i64
  %72 = getelementptr double* %0, i64 %71
  %73 = bitcast double* %72 to <1 x double>*
  %74 = load <1 x double>* %73, align 8
  %75 = add i32 %5, 4
  %76 = sext i32 %75 to i64
  %77 = getelementptr double* %0, i64 %76
  %78 = bitcast double* %77 to <1 x double>*
  %79 = load <1 x double>* %78, align 8
  %80 = getelementptr double* %0, i64 %39
  %81 = bitcast double* %80 to <1 x double>*
  %82 = load <1 x double>* %81, align 8
  %83 = or i64 %39, 1
  %84 = getelementptr double* %0, i64 %83
  %85 = bitcast double* %84 to <1 x double>*
  %86 = load <1 x double>* %85, align 8
  %87 = add i32 %36, 2
  %88 = sext i32 %87 to i64
  %89 = getelementptr double* %0, i64 %88
  %90 = bitcast double* %89 to <1 x double>*
  %91 = load <1 x double>* %90, align 8
  %92 = add i32 %36, 3
  %93 = sext i32 %92 to i64
  %94 = getelementptr double* %0, i64 %93
  %95 = bitcast double* %94 to <1 x double>*
  %96 = load <1 x double>* %95, align 8
  %97 = add i32 %36, 4
  %98 = sext i32 %97 to i64
  %99 = getelementptr double* %0, i64 %98
  %100 = bitcast double* %99 to <1 x double>*
  %101 = load <1 x double>* %100, align 8
  %102 = getelementptr double* %0, i64 %42
  %103 = bitcast double* %102 to <1 x double>*
  %104 = load <1 x double>* %103, align 8
  %105 = add i32 %37, 1
  %106 = sext i32 %105 to i64
  %107 = getelementptr double* %0, i64 %106
  %108 = bitcast double* %107 to <1 x double>*
  %109 = load <1 x double>* %108, align 8
  %110 = add i32 %37, 2
  %111 = sext i32 %110 to i64
  %112 = getelementptr double* %0, i64 %111
  %113 = bitcast double* %112 to <1 x double>*
  %114 = load <1 x double>* %113, align 8
  %115 = add i32 %37, 3
  %116 = sext i32 %115 to i64
  %117 = getelementptr double* %0, i64 %116
  %118 = bitcast double* %117 to <1 x double>*
  %119 = load <1 x double>* %118, align 8
  %120 = add i32 %37, 4
  %121 = sext i32 %120 to i64
  %122 = getelementptr double* %0, i64 %121
  %123 = bitcast double* %122 to <1 x double>*
  %124 = load <1 x double>* %123, align 8
  %125 = getelementptr double* %0, i64 %40
  %126 = bitcast double* %125 to <1 x double>*
  %127 = load <1 x double>* %126, align 8
  %128 = or i64 %40, 1
  %129 = getelementptr double* %0, i64 %128
  %130 = bitcast double* %129 to <1 x double>*
  %131 = load <1 x double>* %130, align 8
  %132 = or i64 %40, 2
  %133 = getelementptr double* %0, i64 %132
  %134 = bitcast double* %133 to <1 x double>*
  %135 = load <1 x double>* %134, align 8
  %136 = or i64 %40, 3
  %137 = getelementptr double* %0, i64 %136
  %138 = bitcast double* %137 to <1 x double>*
  %139 = load <1 x double>* %138, align 8
  %140 = add i32 %38, 4
  %141 = sext i32 %140 to i64
  %142 = getelementptr double* %0, i64 %141
  %143 = bitcast double* %142 to <1 x double>*
  %144 = load <1 x double>* %143, align 8
  %145 = getelementptr double* %2, i64 %61
  %146 = bitcast double* %145 to <1 x double>*
  %147 = load <1 x double>* %146, align 8
  %148 = fmul <1 x double> %7, %147
  %149 = add i32 %5, 2
  %150 = sext i32 %149 to i64
  %151 = getelementptr double* %2, i64 %150
  %152 = bitcast double* %151 to <1 x double>*
  %153 = load <1 x double>* %152, align 8
  %154 = fmul <1 x double> %7, %153
  %155 = add i32 %5, 3
  %156 = sext i32 %155 to i64
  %157 = getelementptr double* %2, i64 %156
  %158 = bitcast double* %157 to <1 x double>*
  %159 = load <1 x double>* %158, align 8
  %160 = fmul <1 x double> %7, %159
  %161 = or i32 %36, 1
  %162 = getelementptr double* %2, i64 %83
  %163 = bitcast double* %162 to <1 x double>*
  %164 = load <1 x double>* %163, align 8
  %165 = fmul <1 x double> %7, %164
  %166 = add i32 %161, 1
  %167 = sext i32 %166 to i64
  %168 = getelementptr double* %2, i64 %167
  %169 = bitcast double* %168 to <1 x double>*
  %170 = load <1 x double>* %169, align 8
  %171 = fmul <1 x double> %7, %170
  %172 = add i32 %161, 2
  %173 = sext i32 %172 to i64
  %174 = getelementptr double* %2, i64 %173
  %175 = bitcast double* %174 to <1 x double>*
  %176 = load <1 x double>* %175, align 8
  %177 = fmul <1 x double> %7, %176
  %178 = getelementptr double* %2, i64 %106
  %179 = bitcast double* %178 to <1 x double>*
  %180 = load <1 x double>* %179, align 8
  %181 = fmul <1 x double> %7, %180
  %182 = add i32 %37, 2
  %183 = sext i32 %182 to i64
  %184 = getelementptr double* %2, i64 %183
  %185 = bitcast double* %184 to <1 x double>*
  %186 = load <1 x double>* %185, align 8
  %187 = fmul <1 x double> %7, %186
  %188 = add i32 %37, 3
  %189 = sext i32 %188 to i64
  %190 = getelementptr double* %2, i64 %189
  %191 = bitcast double* %190 to <1 x double>*
  %192 = load <1 x double>* %191, align 8
  %193 = fmul <1 x double> %7, %192
  %194 = load <1 x double>* %8, align 8
  %195 = load <1 x double>* %10, align 8
  %196 = load <1 x double>* %12, align 8
  %197 = load <1 x double>* %15, align 8
  %198 = load <1 x double>* %19, align 8
  %199 = load <1 x double>* %23, align 8
  %200 = load <1 x double>* %27, align 8
  %201 = load <1 x double>* %31, align 8
  %202 = load <1 x double>* %35, align 8
  %203 = fmul <1 x double> %44, %194
  %204 = fadd <1 x double> %148, %203
  %205 = fmul <1 x double> %47, %195
  %206 = fadd <1 x double> %204, %205
  %207 = fmul <1 x double> %50, %196
  %208 = fadd <1 x double> %206, %207
  %209 = fmul <1 x double> %59, %197
  %210 = fadd <1 x double> %208, %209
  %211 = fmul <1 x double> %64, %198
  %212 = fadd <1 x double> %210, %211
  %213 = fmul <1 x double> %69, %199
  %214 = fadd <1 x double> %212, %213
  %215 = fmul <1 x double> %82, %200
  %216 = fadd <1 x double> %214, %215
  %217 = fmul <1 x double> %86, %201
  %218 = fadd <1 x double> %216, %217
  %219 = fmul <1 x double> %91, %202
  %220 = fadd <1 x double> %218, %219
  %221 = fmul <1 x double> %47, %194
  %222 = fadd <1 x double> %154, %221
  %223 = fmul <1 x double> %50, %195
  %224 = fadd <1 x double> %222, %223
  %225 = fmul <1 x double> %53, %196
  %226 = fadd <1 x double> %224, %225
  %227 = fmul <1 x double> %64, %197
  %228 = fadd <1 x double> %226, %227
  %229 = fmul <1 x double> %69, %198
  %230 = fadd <1 x double> %228, %229
  %231 = fmul <1 x double> %74, %199
  %232 = fadd <1 x double> %230, %231
  %233 = fmul <1 x double> %86, %200
  %234 = fadd <1 x double> %232, %233
  %235 = fmul <1 x double> %91, %201
  %236 = fadd <1 x double> %234, %235
  %237 = fmul <1 x double> %96, %202
  %238 = fadd <1 x double> %236, %237
  %239 = fmul <1 x double> %50, %194
  %240 = fadd <1 x double> %160, %239
  %241 = fmul <1 x double> %53, %195
  %242 = fadd <1 x double> %240, %241
  %243 = fmul <1 x double> %56, %196
  %244 = fadd <1 x double> %242, %243
  %245 = fmul <1 x double> %69, %197
  %246 = fadd <1 x double> %244, %245
  %247 = fmul <1 x double> %74, %198
  %248 = fadd <1 x double> %246, %247
  %249 = fmul <1 x double> %79, %199
  %250 = fadd <1 x double> %248, %249
  %251 = fmul <1 x double> %91, %200
  %252 = fadd <1 x double> %250, %251
  %253 = fmul <1 x double> %96, %201
  %254 = fadd <1 x double> %252, %253
  %255 = fmul <1 x double> %101, %202
  %256 = fadd <1 x double> %254, %255
  %257 = fmul <1 x double> %59, %194
  %258 = fadd <1 x double> %165, %257
  %259 = fmul <1 x double> %64, %195
  %260 = fadd <1 x double> %258, %259
  %261 = fmul <1 x double> %69, %196
  %262 = fadd <1 x double> %260, %261
  %263 = fmul <1 x double> %82, %197
  %264 = fadd <1 x double> %262, %263
  %265 = fmul <1 x double> %86, %198
  %266 = fadd <1 x double> %264, %265
  %267 = fmul <1 x double> %91, %199
  %268 = fadd <1 x double> %266, %267
  %269 = fmul <1 x double> %104, %200
  %270 = fadd <1 x double> %268, %269
  %271 = fmul <1 x double> %109, %201
  %272 = fadd <1 x double> %270, %271
  %273 = fmul <1 x double> %114, %202
  %274 = fadd <1 x double> %272, %273
  %275 = fmul <1 x double> %64, %194
  %276 = fadd <1 x double> %171, %275
  %277 = fmul <1 x double> %69, %195
  %278 = fadd <1 x double> %276, %277
  %279 = fmul <1 x double> %74, %196
  %280 = fadd <1 x double> %278, %279
  %281 = fmul <1 x double> %86, %197
  %282 = fadd <1 x double> %280, %281
  %283 = fmul <1 x double> %91, %198
  %284 = fadd <1 x double> %282, %283
  %285 = fmul <1 x double> %96, %199
  %286 = fadd <1 x double> %284, %285
  %287 = fmul <1 x double> %109, %200
  %288 = fadd <1 x double> %286, %287
  %289 = fmul <1 x double> %114, %201
  %290 = fadd <1 x double> %288, %289
  %291 = fmul <1 x double> %119, %202
  %292 = fadd <1 x double> %290, %291
  %293 = fmul <1 x double> %69, %194
  %294 = fadd <1 x double> %177, %293
  %295 = fmul <1 x double> %74, %195
  %296 = fadd <1 x double> %294, %295
  %297 = fmul <1 x double> %79, %196
  %298 = fadd <1 x double> %296, %297
  %299 = fmul <1 x double> %91, %197
  %300 = fadd <1 x double> %298, %299
  %301 = fmul <1 x double> %96, %198
  %302 = fadd <1 x double> %300, %301
  %303 = fmul <1 x double> %101, %199
  %304 = fadd <1 x double> %302, %303
  %305 = fmul <1 x double> %114, %200
  %306 = fadd <1 x double> %304, %305
  %307 = fmul <1 x double> %119, %201
  %308 = fadd <1 x double> %306, %307
  %309 = fmul <1 x double> %124, %202
  %310 = fadd <1 x double> %308, %309
  %311 = fmul <1 x double> %82, %194
  %312 = fadd <1 x double> %181, %311
  %313 = fmul <1 x double> %86, %195
  %314 = fadd <1 x double> %312, %313
  %315 = fmul <1 x double> %91, %196
  %316 = fadd <1 x double> %314, %315
  %317 = fmul <1 x double> %104, %197
  %318 = fadd <1 x double> %316, %317
  %319 = fmul <1 x double> %109, %198
  %320 = fadd <1 x double> %318, %319
  %321 = fmul <1 x double> %114, %199
  %322 = fadd <1 x double> %320, %321
  %323 = fmul <1 x double> %127, %200
  %324 = fadd <1 x double> %322, %323
  %325 = fmul <1 x double> %131, %201
  %326 = fadd <1 x double> %324, %325
  %327 = fmul <1 x double> %135, %202
  %328 = fadd <1 x double> %326, %327
  %329 = fmul <1 x double> %86, %194
  %330 = fadd <1 x double> %187, %329
  %331 = fmul <1 x double> %91, %195
  %332 = fadd <1 x double> %330, %331
  %333 = fmul <1 x double> %96, %196
  %334 = fadd <1 x double> %332, %333
  %335 = fmul <1 x double> %109, %197
  %336 = fadd <1 x double> %334, %335
  %337 = fmul <1 x double> %114, %198
  %338 = fadd <1 x double> %336, %337
  %339 = fmul <1 x double> %119, %199
  %340 = fadd <1 x double> %338, %339
  %341 = fmul <1 x double> %131, %200
  %342 = fadd <1 x double> %340, %341
  %343 = fmul <1 x double> %135, %201
  %344 = fadd <1 x double> %342, %343
  %345 = fmul <1 x double> %139, %202
  %346 = fadd <1 x double> %344, %345
  %347 = fmul <1 x double> %91, %194
  %348 = fadd <1 x double> %193, %347
  %349 = fmul <1 x double> %96, %195
  %350 = fadd <1 x double> %348, %349
  %351 = fmul <1 x double> %101, %196
  %352 = fadd <1 x double> %350, %351
  %353 = fmul <1 x double> %114, %197
  %354 = fadd <1 x double> %352, %353
  %355 = fmul <1 x double> %119, %198
  %356 = fadd <1 x double> %354, %355
  %357 = fmul <1 x double> %124, %199
  %358 = fadd <1 x double> %356, %357
  %359 = fmul <1 x double> %135, %200
  %360 = fadd <1 x double> %358, %359
  %361 = fmul <1 x double> %139, %201
  %362 = fadd <1 x double> %360, %361
  %363 = fmul <1 x double> %144, %202
  %364 = fadd <1 x double> %362, %363
  store <1 x double> %220, <1 x double>* %146, align 8
  store <1 x double> %238, <1 x double>* %152, align 8
  store <1 x double> %256, <1 x double>* %158, align 8
  store <1 x double> %274, <1 x double>* %163, align 8
  store <1 x double> %292, <1 x double>* %169, align 8
  store <1 x double> %310, <1 x double>* %175, align 8
  store <1 x double> %328, <1 x double>* %179, align 8
  store <1 x double> %346, <1 x double>* %185, align 8
  store <1 x double> %364, <1 x double>* %191, align 8
  ret void
}


